# Mini WebUI para Ollama (HTML + JavaScript)

Una interfaz WebUI minimalista, port치til y completamente offline para interactuar con modelos de **Ollama ejecut치ndose en WSL** (Windows Subsystem for Linux) desde cualquier navegador en Windows.

Este proyecto no usa frameworks, backend ni dependencias externas.  
Toda la l칩gica est치 implementada en un 칰nico archivo `index.html`.

---

## 游 Caracter칤sticas principales

- Selecci칩n de modelos instalados en Ollama.
- Panel informativo con:
  - N칰mero de par치metros (`parameter_size`)
  - Tama침o en disco (GB)
  - Quantization (`Q4`, `Q5`, `Q8`, etc.) + explicaci칩n autom치tica
  - Latencia promedio del **primer token**
- Env칤o de prompts con **streaming palabra por palabra**.
- Bot칩n para **detener streaming**.
- Bot칩n para **limpiar historial**.
- Modo respuesta corta (<10 palabras).
- Modo forzar castellano.
- Modal de ayuda integrado.
- Funciona en Windows / WSL / Linux.
- Completamente offline una vez instalados los modelos.
- Sin dependencias externas.

---

## 游닍 Requisitos del sistema

### Windows
- Windows 10 / 11
- WSL (Ubuntu recomendado)
- Python 3
- Navegador (Chrome/Edge requiere servidor; Firefox puede abrir localmente)

---

## 游냖 Instalaci칩n de WSL

```
wsl --install
```

---

## 游뱄 Instalaci칩n de Ollama dentro de WSL

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

API en:

```
http://localhost:11434
```

---

## 游닌 Descargar modelos

```bash
ollama pull gemma3:4b
ollama pull llama3.2:3b
ollama pull mistral:7b
```

---

## 游깷 Ejecutar la Mini WebUI

```
python3 -m http.server 8000
```

Entrar luego en:

```
http://localhost:8000
```

---

## 游댋 API usada

- `/api/tags`
- `/api/show`
- `/api/generate` (stream)

---

## 游닇 Licencia MIT

Autor: **Rommel Contreras**  
Blog: https://tecnologiacumanesa.blogspot.com/

